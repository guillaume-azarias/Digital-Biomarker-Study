{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcuts:\n",
    "    - Move cell down: .\n",
    "    - Move cell up: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdO5x8DMJrmm"
   },
   "source": [
    "# Anomaly detection using Facebook Prophet:\n",
    "\n",
    "*This notebook is based on the following [tutorial](https://medium.com/analytics-vidhya/time-series-forecast-anomaly-detection-with-facebook-prophet-558136be4b8d) written by Paul Lo. It makes use of the open-source project [Prophet](https://facebook.github.io/prophet/), a forecasting procedure implemented in R and Python, based on the paper of [Taylor and Letham, 2017](https://peerj.com/preprints/3190/).*\n",
    "\n",
    "**Goal of the script:**\n",
    "\n",
    "Here, we aim to the stress-related detect changes related to stress and is based on the analysis of the WESAD dataset.\n",
    "\n",
    "**Motivations to use a forecasting method to detect activity:**\n",
    "\n",
    "Previous works demonstrated the ability to related self-labeled stress status to sensor data acquired by wearable sensors.\n",
    "Here we try a different approach assuming that physiological rythms are altered by stress. We are investigating if a time series forecasting method coupled with anomaly detection provides a more sensitive methods to detect stress-related changes.\n",
    "\n",
    "**Data format**\n",
    "\n",
    "The reader may read the\n",
    "- [UCI website](https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29) (check the website shown below) to download the WESAD dataset \n",
    "- [wesad_readme file](wesad_readme.pdf) and [wesad poster](WESAD poster.pdf), both located together with the WESAD dataset\n",
    "\n",
    "> Questions:\n",
    "> Contact Guillaume Azarias at guillaume.azarias@hotmail.com"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import qgrid\n",
    "\n",
    "# Note that the interactive plot may not work in Jupyter lab, but only in Jupyter Notebook (conflict of javascripts)\n",
    "%matplotlib widget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8FSgHwCa6bi"
   },
   "outputs": [],
   "source": [
    "import fbprophet\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "from fbprophet.plot import plot_cross_validation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rKZsaWFTa9Km",
    "outputId": "15c0d68c-89da-4503-b0c4-98703d9a789a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbprophet.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import itertools\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions from the helper.py\n",
    "from helper import load_ds, df_dev_formater, find_index, df_generator, prophet_fit, prophet_plot, get_outliers, prophet, GridSearch_Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the WESAD data\n",
    "\n",
    "The loading script was modified from [a repository from aganjag](https://github.com/jaganjag/stress_affect_detection/blob/master/prototype.ipynb)\n",
    "\n",
    "The dimensions of the dataset depend on both the device and parameters:\n",
    "\n",
    "|     Device     | Location|Parameter|Acq. frequency|Number of dimensions|Data points (S5)| Duration (S5)|\n",
    "|:---------------|:-------:|:-------:|:------------:|:------------------:|:--------------:|:------------:|\n",
    "|**RespiBAN Pro**|chest    | ACC     |700Hz         |**3**               |4496100         |6'423sec      |\n",
    "|                |         | ECG     |\"             |1                   |                |              |\n",
    "|                |         | EDA     |\"             |1                   |                |              |\n",
    "|                |         | EMG     |\"             |1                   |                |              |\n",
    "|                |         | RESP    |\"             |1                   |                |              |\n",
    "|                |         | TEMP    |\"             |1                   |                |              |\n",
    "|                |         |         |              |                    |                |              |\n",
    "|**Empatica E4** |wrist    | ACC     |32Hz          |**3**               |200256          |6'258sec      |\n",
    "|                |         | BVP     |64Hz          |1                   |400512          |              |\n",
    "|                |         | EDA     |4Hz           |1                   |25032           |              |\n",
    "|                |         | TEMP    |4Hz           |1                   |25032           |              |\n",
    "\n",
    "*Note that ACC is a matrix of 3 dimensions for the 3 spatial dimensions*\n",
    "\n",
    "*'ECG', 'EDA', 'EMG', 'Resp', 'Temp' have each 1 dimension.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_data[subject] = read_data_one_subject(data_set_path, subject)\n",
    "class read_data_of_one_subject:\n",
    "    \"\"\"Read data from WESAD dataset\"\"\"\n",
    "    def __init__(self, path, subject):\n",
    "        self.keys = ['label', 'subject', 'signal']\n",
    "        self.signal_keys = ['wrist', 'chest']\n",
    "        self.chest_sensor_keys = ['ACC', 'ECG', 'EDA', 'EMG', 'Resp', 'Temp']\n",
    "        self.wrist_sensor_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        os.chdir(path) # Change the current working directory to path\n",
    "        os.chdir(subject) # Change the current working directory to path. Why not using data_set_path ?\n",
    "        with open(subject + '.pkl', 'rb') as file: # with will automatically close the file after the nested block of code\n",
    "            data = pickle.load(file, encoding='latin1')\n",
    "        self.data = data\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.data[self.keys[0]]\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        \"\"\"\"\"\"\n",
    "        #label = self.data[self.keys[0]]\n",
    "        assert subject == self.data[self.keys[1]], 'WARNING: Mixing up the data from different persons'\n",
    "        signal = self.data[self.keys[2]]\n",
    "        wrist_data = signal[self.signal_keys[0]]\n",
    "        #wrist_ACC = wrist_data[self.wrist_sensor_keys[0]]\n",
    "        #wrist_ECG = wrist_data[self.wrist_sensor_keys[1]]\n",
    "        return wrist_data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        \"\"\"\"\"\"\n",
    "        assert subject == self.data[self.keys[1]], 'WARNING: Mixing up the data from different persons'\n",
    "        signal = self.data[self.keys[2]]\n",
    "        chest_data = signal[self.signal_keys[1]]\n",
    "        return chest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_path = \"../../Data/WESAD\"\n",
    "subject = 'S5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_data = {}\n",
    "obj_data[subject] = read_data_of_one_subject(data_set_path, subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Workplan:*\n",
    "\n",
    "**A) Exploratory data analysis**\n",
    "\n",
    "    1) Discard for now the ACC data. Preliminary results on other parameters may guide the ways to investigate the accelerometer data\n",
    "    2) Get the study protocol\n",
    "    3) Use rolling.mean() to synchronise the data at the same frequency\n",
    "    4) Synchronise data\n",
    "    5) Include label data if possible\n",
    "    6) Plot data\n",
    "\n",
    "**B) Perform time series forecasting**\n",
    "\n",
    "    1) ADCF test\n",
    "    2) Prophet\n",
    "    3) ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the study protocol\n",
    "*From the wesad_readme.pdf:*\n",
    "\n",
    "The order of the different conditions is defined on the second line in SX_quest.csv. Please refer to [1] for further details on each of the conditions (see Section 3.3 there). Please ignore the elements “bRead”, “fRead”, and “sRead”: these are not relevant for this dataset.\n",
    "The time interval of each condition is defined as start and end time, see the lines 3 and 4 in SX_quest.csv. Time is given in the format [minutes.seconds]. Time is counted from the start of the RespiBAN device’s start of recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Projects/Data/WESAD/S5\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guillaume/Documents/Projects/Data/WESAD/S5/S5_quest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Subj</th>\n",
       "      <th>S5</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "      <th>Unnamed: 26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># ORDER</td>\n",
       "      <td>Base</td>\n",
       "      <td>Fun</td>\n",
       "      <td>Medi 1</td>\n",
       "      <td>TSST</td>\n",
       "      <td>Medi 2</td>\n",
       "      <td>bRead</td>\n",
       "      <td>fRead</td>\n",
       "      <td>sRead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># START</td>\n",
       "      <td>5.37</td>\n",
       "      <td>32</td>\n",
       "      <td>45.43</td>\n",
       "      <td>61</td>\n",
       "      <td>92.15</td>\n",
       "      <td>26.15</td>\n",
       "      <td>41.15</td>\n",
       "      <td>75.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># END</td>\n",
       "      <td>25.55</td>\n",
       "      <td>38.34</td>\n",
       "      <td>52.4</td>\n",
       "      <td>72.05</td>\n",
       "      <td>99.12</td>\n",
       "      <td>27.47</td>\n",
       "      <td>42.45</td>\n",
       "      <td>76.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># PANAS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    # Subj     S5 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6  \\\n",
       "0  # ORDER   Base        Fun     Medi 1       TSST     Medi 2      bRead   \n",
       "1  # START   5.37         32      45.43         61      92.15      26.15   \n",
       "2    # END  25.55      38.34       52.4      72.05      99.12      27.47   \n",
       "3      NaN    NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4  # PANAS      1          1          4          3          1          3   \n",
       "\n",
       "  Unnamed: 7 Unnamed: 8  Unnamed: 9  ...  Unnamed: 17  Unnamed: 18  \\\n",
       "0      fRead      sRead         NaN  ...          NaN          NaN   \n",
       "1      41.15      75.28         NaN  ...          NaN          NaN   \n",
       "2      42.45      76.32         NaN  ...          NaN          NaN   \n",
       "3        NaN        NaN         NaN  ...          NaN          NaN   \n",
       "4          1          1         1.0  ...          4.0          4.0   \n",
       "\n",
       "   Unnamed: 19  Unnamed: 20  Unnamed: 21  Unnamed: 22 Unnamed: 23  \\\n",
       "0          NaN          NaN          NaN          NaN         NaN   \n",
       "1          NaN          NaN          NaN          NaN         NaN   \n",
       "2          NaN          NaN          NaN          NaN         NaN   \n",
       "3          NaN          NaN          NaN          NaN         NaN   \n",
       "4          1.0          1.0          2.0          1.0         3.0   \n",
       "\n",
       "   Unnamed: 24  Unnamed: 25  Unnamed: 26  \n",
       "0          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN  \n",
       "4          1.0          NaN          NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SX_quest_filename = os.getcwd() + '/' + subject + '_quest.csv'\n",
    "print(SX_quest_filename)\n",
    "# bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S2/S2_quest.csv\", header=1, delimiter=';')\n",
    "study_protocol_raw = pd.read_csv(SX_quest_filename, delimiter=';')\n",
    "study_protocol_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.37</td>\n",
       "      <td>25.55</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.00</td>\n",
       "      <td>38.34</td>\n",
       "      <td>Fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.43</td>\n",
       "      <td>52.40</td>\n",
       "      <td>Medi 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.00</td>\n",
       "      <td>72.05</td>\n",
       "      <td>TSST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.15</td>\n",
       "      <td>99.12</td>\n",
       "      <td>Medi 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end    task\n",
       "0   5.37  25.55    Base\n",
       "1  32.00  38.34     Fun\n",
       "2  45.43  52.40  Medi 1\n",
       "3  61.00  72.05    TSST\n",
       "4  92.15  99.12  Medi 2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with the interval of every steps\n",
    "study_protocol = study_protocol_raw.iloc[1:3, 1:6]\n",
    "study_protocol = study_protocol.transpose().astype(float)\n",
    "study_protocol.columns = ['start', 'end']\n",
    "study_protocol['task'] = study_protocol_raw.iloc[0, 1:6].transpose()\n",
    "study_protocol = study_protocol.reset_index(drop=True)\n",
    "study_protocol\n",
    "# study_protocol.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:00:00\n",
      "2020-01-01 01:39:00.120000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 00:00:00.250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 00:00:00.500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 00:00:00.750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 00:00:01.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23756</th>\n",
       "      <td>2020-01-01 01:38:59.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23757</th>\n",
       "      <td>2020-01-01 01:38:59.250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23758</th>\n",
       "      <td>2020-01-01 01:38:59.500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23759</th>\n",
       "      <td>2020-01-01 01:38:59.750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23760</th>\n",
       "      <td>2020-01-01 01:39:00.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23761 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         time  task\n",
       "0     2020-01-01 00:00:00.000   NaN\n",
       "1     2020-01-01 00:00:00.250   NaN\n",
       "2     2020-01-01 00:00:00.500   NaN\n",
       "3     2020-01-01 00:00:00.750   NaN\n",
       "4     2020-01-01 00:00:01.000   NaN\n",
       "...                       ...   ...\n",
       "23756 2020-01-01 01:38:59.000   NaN\n",
       "23757 2020-01-01 01:38:59.250   NaN\n",
       "23758 2020-01-01 01:38:59.500   NaN\n",
       "23759 2020-01-01 01:38:59.750   NaN\n",
       "23760 2020-01-01 01:39:00.000   NaN\n",
       "\n",
       "[23761 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with the time formatted as datetime\n",
    "# Note that the frequency chosen was 4Hz to match the lowest frequency of acquisition (250ms)\n",
    "total_duration = study_protocol.end.max()\n",
    "data = pd.DataFrame()\n",
    "begin_df = datetime.datetime(2020, 1, 1) # For reading convenience\n",
    "print(begin_df)\n",
    "end_df = begin_df + timedelta(minutes=int(total_duration)) + timedelta(seconds=total_duration-int(total_duration))\n",
    "print(end_df)\n",
    "data['time'] = pd.date_range(start=begin_df, end=end_df, freq='250L').to_pydatetime().tolist()\n",
    "data['task'] = np.nan\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:00:00\n",
      "2020-01-01 00:05:00.370000\n",
      "2020-01-01 00:25:00.550000\n"
     ]
    }
   ],
   "source": [
    "row = 0\n",
    "# Annotation of the data with the task\n",
    "print(begin_df)\n",
    "begin_state = study_protocol.iloc[row, 0]\n",
    "begin = begin_df + timedelta(minutes=int(begin_state)) + timedelta(seconds=begin_state-int(begin_state))\n",
    "print(begin)\n",
    "end_state = study_protocol.iloc[row, 1]\n",
    "end = begin_df + timedelta(minutes=int(end_state)) + timedelta(seconds=end_state-int(end_state))\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Iterate data annotation according to the study protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353f213e25f74e33a86a46bbc4a489f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defau…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.loc[(data['time'] >= begin) & (data['time'] <= end), 'task'] = study_protocol.iloc[row, 2]\n",
    "qgrid_widget = qgrid.show_grid(data, show_toolbar=True)\n",
    "qgrid.show_grid(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the wrist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S3/S3_quest.csv\", header=1, delimiter=';')\n",
    "bp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S4/S4_quest.csv\", header=1, delimiter=';')\n",
    "bp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S5/S5_quest.csv\", delimiter=';')\n",
    "# qgrid_widget = qgrid.show_grid(bp_data, show_toolbar=True)\n",
    "# qgrishow_gridrid(bp_data)\n",
    "bp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S5/S5_quest.csv\", header=1, delimiter=';')\n",
    "# qgrid_widget = qgrid.show_grid(bp_data, show_toolbar=True)\n",
    "# qgrishow_gridrid(bp_data)\n",
    "bp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "6423/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_data = pd.read_csv(\"/Users/guillaume/Documents/Projects/Data/WESAD/S6/S6_quest.csv\", header=1, delimiter=';')\n",
    "bp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wrist_data_dict = obj_data[subject].get_wrist_data()\n",
    "wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "print(wrist_dict_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# blood volume pulse (BVP, 64Hz)\n",
    "BVP_data = wrist_data_dict['BVP'].flatten()\n",
    "# type(BVP_data)\n",
    "df_BVP = pd.DataFrame(data=BVP_data, columns=['BVP'])\n",
    "type(df_BVP_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electrodermal activity and Temperature (EDA, TEMP, 4Hz)\n",
    "wrist_data = np.concatenate((wrist_data_dict['EDA'], wrist_data_dict['TEMP']), axis=1)\n",
    "# type(wrist_data)\n",
    "df_EDA_TEMP = pd.DataFrame(data=wrist_data, columns=['EDA', 'TEMP'])\n",
    "type(df_EDA_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BVP = df_BVP.resample(sampling_period_st).pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data\n",
    "qgrid_widget = qgrid.show_grid(df_EDA_TEMP, show_toolbar=True)\n",
    "qgrid.show_grid(df_EDA_TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the chest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chest_data_dict = obj_data[subject].get_chest_data()\n",
    "chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "print(chest_dict_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to an np array\n",
    "# chest_data = np.concatenate((chest_data_dict['ACC'], chest_data_dict['ECG'], chest_data_dict['EDA'], chest_data_dict['EMG'], chest_data_dict['Resp'], chest_data_dict['Temp']), axis=1)\n",
    "chest_data = np.concatenate((chest_data_dict['ECG'], chest_data_dict['EDA'], chest_data_dict['EMG'], chest_data_dict['Resp'], chest_data_dict['Temp']), axis=1)\n",
    "type(chest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the np array into a pandas DataFrame\n",
    "df = pd.DataFrame(data=chest_data, columns=['ECG', 'EDA', 'EMG', 'Resp', 'Temp'])\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data\n",
    "qgrid_widget = qgrid.show_grid(df, show_toolbar=True)\n",
    "qgrid.show_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of Labels ==> 8 ; 0 = not defined / transient, 1 = baseline, 2 = stress, 3 = amusement,\n",
    "# 4 = meditation, 5/6/7 = should be ignored in this dataset\n",
    "labels = {}\n",
    "labels[subject] = obj_data[subject].get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(labels)\n",
    "# len(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "labels_dict = obj_data[subject].get_labels()\n",
    "labels_dict_length = {key: len(value) for key, value in labels_dict.items()}\n",
    "printint(labels_dict_length)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert the list to an np array\n",
    "# chest_data = np.concatenate((chest_data_dict['ACC'], chest_data_dict['ECG'], chest_data_dict['EDA'], chest_data_dict['EMG'], chest_data_dict['Resp'], chest_data_dict['Temp']), axis=1)\n",
    "# labels_data = np.concatenate((chest_data_dict['ECG'], chest_data_dict['EDA'], chest_data_dict['EMG'], chest_data_dict['Resp'], chest_data_dict['Temp']), axis=1)\n",
    "type(labels_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert the np array into a pandas DataFrame\n",
    "df_labels = pd.DataFrame(data=chest_data, columns=['ECG', 'EDA', 'EMG', 'Resp', 'Temp'])\n",
    "type(df_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Show the data\n",
    "qgrid_widget = qgrid.show_grid(df_labels, show_toolbar=True)\n",
    "qgrid.show_grid(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snippet plot\n",
    "# Silly example data\n",
    "bp_x = np.linspace(0, 2*np.pi, num=40, endpoint=True)\n",
    "bp_y = np.sin(bp_x)\n",
    "\n",
    "# Make the plot\n",
    "plt.plot(bp_x, bp_y, linewidth=3, linestyle=\"--\",\n",
    "         color=\"blue\", label=r\"Legend label $\\sin(x)$\")\n",
    "plt.xlabel(r\"Description of $x$ coordinate (units)\")\n",
    "plt.ylabel(r\"Description of $y$ coordinate (units)\")\n",
    "plt.title(r\"Title here (remove for papers)\")\n",
    "plt.xlim(0, 2*np.pi)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_one(chest_data_dict, idx, l_condition=0):\n",
    "    ecg_data = chest_data_dict[\"ECG\"][idx].flatten()\n",
    "    ecg_features = extract_mean_std_features(ecg_data, label=l_condition)\n",
    "    #print(ecg_features.shape)\n",
    "\n",
    "    eda_data = chest_data_dict[\"EDA\"][idx].flatten()\n",
    "    eda_features = extract_mean_std_features(eda_data, label=l_condition)\n",
    "    #print(eda_features.shape)\n",
    "\n",
    "    emg_data = chest_data_dict[\"EMG\"][idx].flatten()\n",
    "    emg_features = extract_mean_std_features(emg_data, label=l_condition)\n",
    "    #print(emg_features.shape)\n",
    "\n",
    "    temp_data = chest_data_dict[\"Temp\"][idx].flatten()\n",
    "    temp_features = extract_mean_std_features(temp_data, label=l_condition)\n",
    "    #print(temp_features.shape)\n",
    "\n",
    "    baseline_data = np.hstack((eda_features, temp_features, ecg_features, emg_features))\n",
    "    #print(len(baseline_data))\n",
    "    label_array = np.full(len(baseline_data), l_condition)\n",
    "    #print(label_array.shape)\n",
    "    #print(baseline_data.shape)\n",
    "    baseline_data = np.column_stack((baseline_data, label_array))\n",
    "    #print(baseline_data.shape)\n",
    "    return baseline_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute():\n",
    "#     data_set_path = \"/media/jac/New Volume/Datasets/WESAD\"\n",
    "    data_set_path = \"../../../Data/WESAD\"\n",
    "    file_path = \"ecg.txt\"\n",
    "    subject = 'S3' # Why defining subject here since it is defined 6 lines later in a loop ?\n",
    "    obj_data = {}\n",
    "    labels = {}\n",
    "    all_data = {}\n",
    "    subs = [2, 3, 4, 5, 6]\n",
    "    for i in subs:\n",
    "        subject = 'S' + str(i)\n",
    "        print(\"Reading data\", subject)\n",
    "        obj_data[subject] = read_data_one_subject(data_set_path, subject)\n",
    "        labels[subject] = obj_data[subject].get_labels()\n",
    "\n",
    "        wrist_data_dict = obj_data[subject].get_wrist_data()\n",
    "        wrist_dict_length = {key: len(value) for key, value in wrist_data_dict.items()}\n",
    "\n",
    "        chest_data_dict = obj_data[subject].get_chest_data()\n",
    "        chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "        print(chest_dict_length)\n",
    "        chest_data = np.concatenate((chest_data_dict['ACC'], chest_data_dict['ECG'], chest_data_dict['EDA'],\n",
    "                                     chest_data_dict['EMG'], chest_data_dict['Resp'], chest_data_dict['Temp']), axis=1)\n",
    "        # Get labels\n",
    "\n",
    "\n",
    "        # 'ACC' : 3, 'ECG' 1: , 'EDA' : 1, 'EMG': 1, 'RESP': 1, 'Temp': 1  ===> Total dimensions : 8\n",
    "        # No. of Labels ==> 8 ; 0 = not defined / transient, 1 = baseline, 2 = stress, 3 = amusement,\n",
    "        # 4 = meditation, 5/6/7 = should be ignored in this dataset\n",
    "\n",
    "        # Do for each subject\n",
    "        baseline = np.asarray([idx for idx, val in enumerate(labels[subject]) if val == 1])\n",
    "        # print(\"Baseline:\", chest_data_dict['ECG'][baseline].shape)\n",
    "        # print(baseline.shape)\n",
    "\n",
    "        stress = np.asarray([idx for idx, val in enumerate(labels[subject]) if val == 2])\n",
    "        # print(stress.shape)\n",
    "\n",
    "        amusement = np.asarray([idx for idx, val in enumerate(labels[subject]) if val == 3])\n",
    "        # print(amusement.shape)\n",
    "\n",
    "        baseline_data = extract_one(chest_data_dict, baseline, l_condition=1)\n",
    "        stress_data = extract_one(chest_data_dict, stress, l_condition=2)\n",
    "        amusement_data = extract_one(chest_data_dict, amusement, l_condition=3)\n",
    "\n",
    "        full_data = np.vstack((baseline_data, stress_data, amusement_data))\n",
    "        print(\"One subject data\", full_data.shape)\n",
    "        all_data[subject] = full_data\n",
    "\n",
    "    i = 0\n",
    "    for k, v in all_data.items():\n",
    "        if i == 0:\n",
    "            data = all_data[k]\n",
    "            i += 1\n",
    "        print(all_data[k].shape)\n",
    "        data = np.vstack((data, all_data[k]))\n",
    "\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "ecg, eda = chest_data_dict['ECG'], chest_data_dict['EDA']\n",
    "x = [i for i in range(len(baseline))]\n",
    "for one in baseline:\n",
    "    x = [i for i in range(99)]\n",
    "    plt.plot(x, ecg[one:100])\n",
    "    break\n",
    "# \"\"\"\n",
    "\n",
    "x = [i for i in range(10000)]\n",
    "plt.plot(x, chest_data_dict['ECG'][:10000])\n",
    "plt.show()\n",
    "\n",
    "# BASELINE\n",
    "\n",
    "                                   [ecg_features[k] for k in ecg_features.keys()])\n",
    "\n",
    "ecg = nk.ecg_process(ecg=ecg_data, rsp=chest_data_dict['Resp'][baseline].flatten(), sampling_rate=700)\n",
    "print(os.getcwd())\n",
    "\n",
    "# \"\"\"\n",
    "recur_print\n",
    "print(type(ecg))\n",
    "print(ecg.keys())\n",
    "for k in ecg.keys():\n",
    "    print(k)\n",
    "    for i in ecg[k].keys():\n",
    "        print(i)\n",
    "    \n",
    "resp = nk.eda_process(eda=chest_data_dict['EDA'][baseline].flatten(), sampling_rate=700)\n",
    "resp = nk.rsp_process(chest_data_dict['Resp'][baseline].flatten(), sampling_rate=700)\n",
    "for k in resp.keys():\n",
    "    print(k)\n",
    "    for i in resp[k].keys():\n",
    "        print(i)\n",
    "    \n",
    "# For baseline, compute mean, std, for each 700 samples. (1 second values)\n",
    "file_path = os.getcwd()\n",
    "with open(file_path, \"w\") as file:\n",
    "    #file.write(str(ecg['df']))\n",
    "    file.write(str(ecg['ECG']['HRV']['RR_Intervals']))\n",
    "    file.write(\"...\")\n",
    "    file.write(str(ecg['RSP']))\n",
    "    #file.write(\"RESP................\")\n",
    "    #file.write(str(resp['RSP']))\n",
    "    #file.write(str(resp['df']))\n",
    "    #print(type(ecg['ECG']['HRV']['RR_Intervals']))\n",
    "    #file.write(str(ecg['ECG']['Cardiac_Cycles']))\n",
    "    #print(type(ecg['ECG']['Cardiac_Cycles']))\n",
    "    #file.write(ecg['ECG']['Cardiac_Cycles'].to_csv())\n",
    "    \n",
    "    \n",
    "# Plot the processed dataframe, normalizing all variables for viewing purpose\n",
    "# \"\"\"\n",
    "# \"\"\"\n",
    "bio = nk.bio_process(ecg=chest_data_dict[\"ECG\"][baseline].flatten(), rsp=chest_data_dict['Resp'][baseline].flatten(), eda=chest_data_dict[\"EDA\"][baseline].flatten(), sampling_rate=700)\n",
    "nk.z_score(bio[\"df\"]).plot()\n",
    "print(bio[\"ECG\"].keys())\n",
    "print(bio[\"EDA\"].keys())\n",
    "print(bio[\"RSP\"].keys())\n",
    "#ECG\n",
    "print(bio[\"ECG\"][\"HRV\"])\n",
    "print(bio[\"ECG\"][\"R_Peaks\"])\n",
    "#EDA\n",
    "print(bio[\"EDA\"][\"SCR_Peaks_Amplitudes\"])\n",
    "print(bio[\"EDA\"][\"SCR_Onsets\"])\n",
    "#RSP\n",
    "print(bio[\"RSP\"][\"Cycles_Onsets\"])\n",
    "print(bio[\"RSP\"][\"Cycles_Length\"])\n",
    "# \"\"\"\n",
    "print(\"Read data file\")\n",
    "#Flow: Read data for all subjects -> Extract features (Preprocessing) -> Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_path = \"../../../Data/WESAD\"\n",
    "subject = 'S4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_data = {}\n",
    "obj_data[subject] = read_data_of_one_subject(data_set_path, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chest_data_dict = obj_data[subject].get_chest_data()\n",
    "chest_dict_length = {key: len(value) for key, value in chest_data_dict.items()}\n",
    "print(chest_dict_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "labels = obj_data[subject].get_labels()\n",
    "baseline = np.asarray([idx for idx,val in enumerate(labels) if val == 1])\n",
    "#print(baseline)\n",
    "\n",
    "print(\"Baseline:\", chest_data_dict['ECG'][baseline].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bio = nk.bio_process(ecg=chest_data_dict[\"ECG\"][baseline].flatten(), rsp=chest_data_dict['Resp'][baseline].flatten(), eda=chest_data_dict[\"EDA\"][baseline].flatten(), sampling_rate=700)\n",
    "nk.z_score(bio[\"df\"]).plot()\n",
    "\"\"\"print(bio[\"ECG\"].keys())\n",
    "print(bio[\"EDA\"].keys())\n",
    "print(bio[\"RSP\"].keys())\n",
    "\n",
    "#ECG\n",
    "print(bio[\"ECG\"][\"HRV\"])\n",
    "print(bio[\"ECG\"][\"R_Peaks\"])\n",
    "\n",
    "#EDA\n",
    "print(bio[\"EDA\"][\"SCR_Peaks_Amplitudes\"])\n",
    "print(bio[\"EDA\"][\"SCR_Onsets\"])\n",
    "\n",
    "#RSP\n",
    "print(bio[\"RSP\"][\"Cycles_Onsets\"])\n",
    "print(bio[\"RSP\"][\"Cycles_Length\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to display the dataframe with qgrid:\n",
    "\n",
    "Check the [quantopian link](https://github.com/quantopian/qgrid).\n",
    "\n",
    "```python\n",
    "import qgrid\n",
    "qgrid_widget = qgrid_widget.show_grid(df, show_toolbar=True)\n",
    "qgrid_widget\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics in Time Series Modelling\n",
    "https://towardsdatascience.com/descriptive-statistics-in-time-series-modelling-db6ec569c0b8\n",
    "\n",
    "Stationarity\n",
    "A time series is said to be stationary if it doesn’t increase or decrease with time linearly or exponentially(no trends), and if it doesn’t show any kind of repeating patterns(no seasonality). Mathematically, this is described as having constant mean and constant variance over time. Along, with variance, the autocovariance should also not be a function of time. If you have forgotten what mean and variance are: mean is the average of the data and variance is the average squared distance from the mean.\n",
    "\n",
    "Sometimes, it’s even difficult to interpret the rolling mean visually so we take the help of statistical tests to identify this, one such being Augmented Dickey Fuller Test. ADCF Test is implemented using statsmodels in python which performs a classic null hypothesis test and returns a p-value.\n",
    "Interpretation of null hypothesis test: If p-value is less than 0.05 (p-value: low), we reject the null hypothesis and assume that the data is stationary. But if the p-value is more than 0.05 (p-value: high), then we fail to reject the null hypothesis and determine the data to be non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Random Forest Classifier (from jaganjag Github)\n",
    "\n",
    "*Not sure if it would be relevant but keeping the code for completeness of the repo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = execute()\n",
    "    print(data.shape)\n",
    "    X = data[:, :16]  # 16 features\n",
    "    y = data[:, 16]\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(X, y,\n",
    "                                                                                test_size=0.25)\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=5, oob_score=True)\n",
    "    clf.fit(X, y)\n",
    "    print(clf.feature_importances_)\n",
    "    # print(clf.oob_decision_function_)\n",
    "    print(clf.oob_score_)\n",
    "    predictions = clf.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    print(\"M A E: \", np.mean(errors))\n",
    "    print(np.count_nonzero(errors), len(test_labels))\n",
    "    print(\"Accuracy:\", np.count_nonzero(errors)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=6,\n",
    "                            n_informative=3, n_redundant=0,\n",
    "                            random_state=0, shuffle=True)\n",
    "\n",
    "print(X.shape)  # 10000x6\n",
    "print(y.shape)  # 10000\n",
    "\n",
    "# TODO: Feature extraction using sliding window\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y,\n",
    "                                                                            test_size=0.25, random_state=42)\n",
    "# TODO: K-fold cross validation\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=3, oob_score=True\n",
    "                             )\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "#print(clf.oob_decision_function_)\n",
    "print(clf.oob_score_)\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "errors = abs(predictions - test_labels)\n",
    "print(\"M A E: \", round(np.mean(errors), 2))\n",
    "\n",
    "\n",
    "# Visualization\n",
    "feature_list = [1, 2, 3, 4, 5, 6]\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = clf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file='tree.dot', feature_names=feature_list, rounded=True, precision=1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "#graph.write_png('tree_.png')\n",
    "\n",
    "# TODO: Confusion matrix, Accuracy\n",
    "\n",
    "\n",
    "# GMM\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full')\n",
    "gmm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to downsample the dataset, run a GridSearch, sort the best model according to the mean average percentage error\n",
    "\n",
    "* Downsampling of dataset: Pick 10 days in a device-specific dataset and will run the GridSearch. Allow to run the algorithm on all device-specific dataframes.\n",
    "* GridSearch trying Prophet with different training periods (8, 10 or 12 training days). This was the most critical parameter affecting the mean average percentage error (mape).\n",
    "* Sort the Prophet model according to the mape. Save the best model with graph and a dataframe containing the prediction and actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10 # Limit to 10 predictions per device.\n",
    "pred_duration = 12 # 12-day prediction\n",
    "\n",
    "for dev_nb in range(1,52):\n",
    "    device_nb = str('{:02d}'.format(dev_nb))\n",
    "    # Load the device-specific dataframe.\n",
    "    assert isinstance(device_nb, str) and len(device_nb)==2 and sum(d.isdigit() for d in device_nb)==2, 'WARNING: device_nb must be a string of 2-digits!'\n",
    "    assert int(device_nb)>=1 and int(device_nb)<=51, 'This device does not belong to the dataframe'\n",
    "    device, df_dev = load_ds(device_nb)\n",
    "    # Convert the variable device from a np.array to a string\n",
    "    regex = re.compile('[^A-Za-z0-9]')\n",
    "    device = regex.sub('', str(device))\n",
    "    \n",
    "    # Create a dataframe with the dates to use\n",
    "    dates = pd.DataFrame(columns={'date_minus_12',\n",
    "                                  'date_minus_10',\n",
    "                                  'date_minus_8',\n",
    "                                  'date_predict'})\n",
    "    dates = dates[['date_minus_12', 'date_minus_10', 'date_minus_8', 'date_predict']]\n",
    "    # List of unique dates in the dataframe\n",
    "    dates['date_minus_12'] = df_dev['ds'].unique().strftime('%Y-%m-%d')\n",
    "    dates = dates.drop_duplicates(subset=['date_minus_12'])\n",
    "    dates = dates.reset_index(drop=True)\n",
    "    # Fill the other columns and drop the 12 last columns\n",
    "    dates['date_minus_10'] = dates.iloc[2:, 0].reset_index(drop=True)\n",
    "    dates['date_minus_8'] = dates.iloc[4:, 0].reset_index(drop=True)\n",
    "    dates['date_predict'] = dates.iloc[12:, 0].reset_index(drop=True)\n",
    "    dates = dates[:-pred_duration] # Drop the 12 last rows\n",
    "    \n",
    "    # Keep only the dates with at least 12 training days\n",
    "    dates['Do_It'] = 'Do not'\n",
    "    dates['dm_12_c'] = np.nan\n",
    "    \n",
    "    for r in range(dates.shape[0]):\n",
    "        # Calculate the date_predict - pred_duration\n",
    "        date_predict = dates.iloc[r, 3]\n",
    "        date_predict = datetime.strptime(date_predict, \"%Y-%m-%d\")\n",
    "\n",
    "        date_minus_12_check = date_predict + timedelta(days=-pred_duration)\n",
    "        date_minus_12_check = datetime.strftime(date_minus_12_check, \"%Y-%m-%d\")\n",
    "        \n",
    "        # Tag the date_predict that have at least 12 training days\n",
    "        if date_minus_12_check in dates.date_predict.values or r<=11:\n",
    "            dates.iloc[r, 4] = 'yes'\n",
    "\n",
    "    dates = dates[dates.Do_It == 'yes']\n",
    "    dates.drop(['Do_It', 'dm_12_c'], axis=1)\n",
    "    \n",
    "    # Downsampling\n",
    "    if dates.shape[0]>n_samples:\n",
    "        dates = dates.sample(n=n_samples, replace=False)\n",
    "\n",
    "    # GridSearch over the (down-sampled) dataset:\n",
    "    start_time = time.time()\n",
    "    mape_table_full = pd.DataFrame()\n",
    "    \n",
    "    for r in range(dates.shape[0]):\n",
    "        # Parameters of the Grid\n",
    "        prophet_grid = {'df_dev' : [df_dev],\n",
    "                        'device' : [device],\n",
    "                        'parameter' : ['co2'],\n",
    "                        'begin' : dates.iloc[r, :3].tolist(),\n",
    "                        'end' : [dates.iloc[r, 3]],\n",
    "                        'sampling_period_min' : [1],\n",
    "                        'graph' : [1],\n",
    "                        'predict_day' : [1],\n",
    "                        'interval_width' : [0.6],\n",
    "                        'changepoint_prior_scale' : [0.01, 0.005], # list(np.arange(0.01,30,1).tolist()),\n",
    "                        'daily_fo' : [3],\n",
    "            #             'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "                           }\n",
    "\n",
    "        # Run GridSearch_Prophet\n",
    "        mape_table = GridSearch_Prophet(list(ParameterGrid(prophet_grid)), metric='mape')\n",
    "        mape_table_full = mape_table_full.append(mape_table)\n",
    "\n",
    "        end_time = time.time()\n",
    "        dur_min = int((end_time - start_time)/60)\n",
    "        \n",
    "        print('Time elapsed: '+ str(dur_min) + \" minutes.\")\n",
    "\n",
    "        # Save the best model\n",
    "        print('Saving the best model')\n",
    "        \n",
    "        best_model = {'df_dev' : [df_dev],\n",
    "                      'device' : [mape_table.iloc[0, 0]],\n",
    "                      'parameter' : [mape_table.iloc[0, 1]],\n",
    "                      'begin' : [mape_table.iloc[0, 2]],\n",
    "                      'end' : [mape_table.iloc[0, 3]],\n",
    "                      'sampling_period_min' : [mape_table.iloc[0, 4]],\n",
    "                      'graph' : [1],\n",
    "                      'predict_day' : [1],\n",
    "                      'interval_width' : [mape_table.iloc[0, 5]],\n",
    "                      'changepoint_prior_scale' : [mape_table.iloc[0, 7]], # list(np.arange(0.01,30,1).tolist()),\n",
    "                      'daily_fo' : [mape_table.iloc[0, 6]],\n",
    "#                       'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "                           }\n",
    "\n",
    "        # Run GridSearch_Prophet on the best model\n",
    "        mape_table = GridSearch_Prophet(list(ParameterGrid(best_model)), metric='mape')\n",
    "\n",
    "        end_time = time.time()\n",
    "        dur_min = int((end_time - start_time)/60)\n",
    "        print('Full analysis completed in '+ str(dur_min) + ' minutes.')\n",
    "\n",
    "    # Save the full table of mape_table\n",
    "    # Store the complete mape_table if this is the last prediction\n",
    "    folder_name = '/Users/guillaume/Documents/DS2020/Caru/caru/data/processed/'\n",
    "    mape_table_name = folder_name + re.sub(\"[']\", '', str(mape_table.iloc[0, 0])) + '_mape_table_full.csv'\n",
    "    mape_table_full.to_csv(mape_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GridSearch over dataset:\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Parameters\n",
    "prophet_grid = {'df_dev' : [df_dev],\n",
    "                'device' : [device],\n",
    "                'parameter' : ['co2'],\n",
    "                'begin' : ['2019-03-20', '2019-03-22'],\n",
    "                'end' : ['2019-04-01'],\n",
    "                'sampling_period_min' : [1],\n",
    "                'graph' : [1],\n",
    "                'predict_day' : [1],\n",
    "                'interval_width' : [0.6],\n",
    "                'changepoint_prior_scale' : [0.01], # list(np.arange(0.01,30,1).tolist()),\n",
    "                'daily_fo' : [3, 6, 9],\n",
    "#                 'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "               }\n",
    "\n",
    "# Run GridSearch_Prophet\n",
    "mape_table = GridSearch_Prophet(list(ParameterGrid(prophet_grid)), metric='mape')\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "dur_min = int((end_time - start_time)/60)\n",
    "\n",
    "print('GridSearch finished '+ str(dur_min) + \" minutes.\")\n",
    "print('Saving the best model')\n",
    "\n",
    "best_model = {'df_dev' : [df_dev],\n",
    "                'device' : [mape_table.iloc[0, 0]],\n",
    "                'parameter' : [mape_table.iloc[0, 1]],\n",
    "                'begin' : [mape_table.iloc[0, 2]],\n",
    "                'end' : [mape_table.iloc[0, 3]],\n",
    "                'sampling_period_min' : [mape_table.iloc[0, 4]],\n",
    "                'graph' : [1],\n",
    "                'predict_day' : [1],\n",
    "                'interval_width' : [mape_table.iloc[0, 5]],\n",
    "                'changepoint_prior_scale' : [mape_table.iloc[0, 7]], # list(np.arange(0.01,30,1).tolist()),\n",
    "                'daily_fo' : [mape_table.iloc[0, 6]],\n",
    "#                 'holidays_prior_scale' : list((1000,100,10,1,0.1)),\n",
    "               }\n",
    "\n",
    "# Run GridSearch_Prophet on the p\n",
    "mape_table = GridSearch_Prophet(list(ParameterGrid(best_model)), metric='mape')\n",
    "\n",
    "end_time = time.time()\n",
    "dur_min = int((end_time - start_time)/60)\n",
    "print('Best model saved in '+ str(dur_min) + ' minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "prophet_anomaly_detection",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Time Series Analysis",
   "language": "python",
   "name": "time_series_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.215px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
